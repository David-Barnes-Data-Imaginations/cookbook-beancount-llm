{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.6685236768802229,
  "eval_steps": 500,
  "global_step": 60,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.011142061281337047,
      "grad_norm": 3.1377501487731934,
      "learning_rate": 0.0,
      "loss": 2.4403066635131836,
      "step": 1
    },
    {
      "epoch": 0.022284122562674095,
      "grad_norm": 3.216057300567627,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 2.4970641136169434,
      "step": 2
    },
    {
      "epoch": 0.033426183844011144,
      "grad_norm": 2.793320417404175,
      "learning_rate": 6.666666666666667e-05,
      "loss": 2.402686595916748,
      "step": 3
    },
    {
      "epoch": 0.04456824512534819,
      "grad_norm": 2.433971643447876,
      "learning_rate": 0.0001,
      "loss": 2.2492048740386963,
      "step": 4
    },
    {
      "epoch": 0.055710306406685235,
      "grad_norm": 3.021827220916748,
      "learning_rate": 0.00013333333333333334,
      "loss": 1.9732826948165894,
      "step": 5
    },
    {
      "epoch": 0.06685236768802229,
      "grad_norm": 1.7185447216033936,
      "learning_rate": 0.0001666666666666667,
      "loss": 1.5982913970947266,
      "step": 6
    },
    {
      "epoch": 0.07799442896935933,
      "grad_norm": 1.250927448272705,
      "learning_rate": 0.0002,
      "loss": 1.3590426445007324,
      "step": 7
    },
    {
      "epoch": 0.08913649025069638,
      "grad_norm": 1.1247749328613281,
      "learning_rate": 0.0001962962962962963,
      "loss": 1.2382537126541138,
      "step": 8
    },
    {
      "epoch": 0.10027855153203342,
      "grad_norm": 1.9494376182556152,
      "learning_rate": 0.0001925925925925926,
      "loss": 1.1341724395751953,
      "step": 9
    },
    {
      "epoch": 0.11142061281337047,
      "grad_norm": 1.5091079473495483,
      "learning_rate": 0.00018888888888888888,
      "loss": 1.0332432985305786,
      "step": 10
    },
    {
      "epoch": 0.12256267409470752,
      "grad_norm": 1.2621638774871826,
      "learning_rate": 0.0001851851851851852,
      "loss": 0.9332537055015564,
      "step": 11
    },
    {
      "epoch": 0.13370473537604458,
      "grad_norm": 1.0952184200286865,
      "learning_rate": 0.0001814814814814815,
      "loss": 0.7639951109886169,
      "step": 12
    },
    {
      "epoch": 0.14484679665738162,
      "grad_norm": 1.0490003824234009,
      "learning_rate": 0.00017777777777777779,
      "loss": 0.6473309397697449,
      "step": 13
    },
    {
      "epoch": 0.15598885793871867,
      "grad_norm": 0.67364501953125,
      "learning_rate": 0.00017407407407407408,
      "loss": 0.4961831271648407,
      "step": 14
    },
    {
      "epoch": 0.1671309192200557,
      "grad_norm": 0.5712409019470215,
      "learning_rate": 0.00017037037037037037,
      "loss": 0.45643138885498047,
      "step": 15
    },
    {
      "epoch": 0.17827298050139276,
      "grad_norm": 0.45320722460746765,
      "learning_rate": 0.0001666666666666667,
      "loss": 0.4517861604690552,
      "step": 16
    },
    {
      "epoch": 0.1894150417827298,
      "grad_norm": 0.39730262756347656,
      "learning_rate": 0.00016296296296296295,
      "loss": 0.41988396644592285,
      "step": 17
    },
    {
      "epoch": 0.20055710306406685,
      "grad_norm": 0.3974042236804962,
      "learning_rate": 0.00015925925925925927,
      "loss": 0.4277327060699463,
      "step": 18
    },
    {
      "epoch": 0.2116991643454039,
      "grad_norm": 0.40225160121917725,
      "learning_rate": 0.00015555555555555556,
      "loss": 0.4299084544181824,
      "step": 19
    },
    {
      "epoch": 0.22284122562674094,
      "grad_norm": 0.4542665481567383,
      "learning_rate": 0.00015185185185185185,
      "loss": 0.4935157597064972,
      "step": 20
    },
    {
      "epoch": 0.233983286908078,
      "grad_norm": 0.3603386878967285,
      "learning_rate": 0.00014814814814814815,
      "loss": 0.32275480031967163,
      "step": 21
    },
    {
      "epoch": 0.24512534818941503,
      "grad_norm": 0.3697068393230438,
      "learning_rate": 0.00014444444444444444,
      "loss": 0.4208453595638275,
      "step": 22
    },
    {
      "epoch": 0.2562674094707521,
      "grad_norm": 0.3489595651626587,
      "learning_rate": 0.00014074074074074076,
      "loss": 0.3772830069065094,
      "step": 23
    },
    {
      "epoch": 0.26740947075208915,
      "grad_norm": 0.31058457493782043,
      "learning_rate": 0.00013703703703703705,
      "loss": 0.35479238629341125,
      "step": 24
    },
    {
      "epoch": 0.2785515320334262,
      "grad_norm": 0.318037748336792,
      "learning_rate": 0.00013333333333333334,
      "loss": 0.36003655195236206,
      "step": 25
    },
    {
      "epoch": 0.28969359331476324,
      "grad_norm": 0.3105255961418152,
      "learning_rate": 0.00012962962962962963,
      "loss": 0.34865206480026245,
      "step": 26
    },
    {
      "epoch": 0.3008356545961003,
      "grad_norm": 0.30621594190597534,
      "learning_rate": 0.00012592592592592592,
      "loss": 0.3475291430950165,
      "step": 27
    },
    {
      "epoch": 0.31197771587743733,
      "grad_norm": 0.4916527569293976,
      "learning_rate": 0.00012222222222222224,
      "loss": 0.3322318494319916,
      "step": 28
    },
    {
      "epoch": 0.3231197771587744,
      "grad_norm": 0.2974979877471924,
      "learning_rate": 0.00011851851851851852,
      "loss": 0.35541167855262756,
      "step": 29
    },
    {
      "epoch": 0.3342618384401114,
      "grad_norm": 0.27719950675964355,
      "learning_rate": 0.00011481481481481482,
      "loss": 0.2980923652648926,
      "step": 30
    },
    {
      "epoch": 0.34540389972144847,
      "grad_norm": 0.27135831117630005,
      "learning_rate": 0.00011111111111111112,
      "loss": 0.32155728340148926,
      "step": 31
    },
    {
      "epoch": 0.3565459610027855,
      "grad_norm": 0.2741031050682068,
      "learning_rate": 0.00010740740740740742,
      "loss": 0.34355729818344116,
      "step": 32
    },
    {
      "epoch": 0.36768802228412256,
      "grad_norm": 0.2841119170188904,
      "learning_rate": 0.0001037037037037037,
      "loss": 0.33851680159568787,
      "step": 33
    },
    {
      "epoch": 0.3788300835654596,
      "grad_norm": 0.25638052821159363,
      "learning_rate": 0.0001,
      "loss": 0.34345167875289917,
      "step": 34
    },
    {
      "epoch": 0.38997214484679665,
      "grad_norm": 0.29125356674194336,
      "learning_rate": 9.62962962962963e-05,
      "loss": 0.3287225663661957,
      "step": 35
    },
    {
      "epoch": 0.4011142061281337,
      "grad_norm": 0.281067430973053,
      "learning_rate": 9.25925925925926e-05,
      "loss": 0.2828982472419739,
      "step": 36
    },
    {
      "epoch": 0.41225626740947074,
      "grad_norm": 0.2934192419052124,
      "learning_rate": 8.888888888888889e-05,
      "loss": 0.32597917318344116,
      "step": 37
    },
    {
      "epoch": 0.4233983286908078,
      "grad_norm": 0.30242833495140076,
      "learning_rate": 8.518518518518518e-05,
      "loss": 0.32849618792533875,
      "step": 38
    },
    {
      "epoch": 0.43454038997214484,
      "grad_norm": 0.2913859784603119,
      "learning_rate": 8.148148148148148e-05,
      "loss": 0.3542252779006958,
      "step": 39
    },
    {
      "epoch": 0.4456824512534819,
      "grad_norm": 0.2924736738204956,
      "learning_rate": 7.777777777777778e-05,
      "loss": 0.3114124536514282,
      "step": 40
    },
    {
      "epoch": 0.4568245125348189,
      "grad_norm": 0.26276296377182007,
      "learning_rate": 7.407407407407407e-05,
      "loss": 0.28526613116264343,
      "step": 41
    },
    {
      "epoch": 0.467966573816156,
      "grad_norm": 0.27576205134391785,
      "learning_rate": 7.037037037037038e-05,
      "loss": 0.3144441843032837,
      "step": 42
    },
    {
      "epoch": 0.479108635097493,
      "grad_norm": 0.265067994594574,
      "learning_rate": 6.666666666666667e-05,
      "loss": 0.3161478638648987,
      "step": 43
    },
    {
      "epoch": 0.49025069637883006,
      "grad_norm": 0.3150531053543091,
      "learning_rate": 6.296296296296296e-05,
      "loss": 0.3363626003265381,
      "step": 44
    },
    {
      "epoch": 0.5013927576601671,
      "grad_norm": 0.29829519987106323,
      "learning_rate": 5.925925925925926e-05,
      "loss": 0.31364864110946655,
      "step": 45
    },
    {
      "epoch": 0.5125348189415042,
      "grad_norm": 0.2555684745311737,
      "learning_rate": 5.555555555555556e-05,
      "loss": 0.3238900303840637,
      "step": 46
    },
    {
      "epoch": 0.5236768802228412,
      "grad_norm": 0.2575891315937042,
      "learning_rate": 5.185185185185185e-05,
      "loss": 0.3096787631511688,
      "step": 47
    },
    {
      "epoch": 0.5348189415041783,
      "grad_norm": 0.25083911418914795,
      "learning_rate": 4.814814814814815e-05,
      "loss": 0.3090279698371887,
      "step": 48
    },
    {
      "epoch": 0.5459610027855153,
      "grad_norm": 0.2717098891735077,
      "learning_rate": 4.4444444444444447e-05,
      "loss": 0.3247006833553314,
      "step": 49
    },
    {
      "epoch": 0.5571030640668524,
      "grad_norm": 0.24399662017822266,
      "learning_rate": 4.074074074074074e-05,
      "loss": 0.2801583707332611,
      "step": 50
    },
    {
      "epoch": 0.5682451253481894,
      "grad_norm": 0.2580074369907379,
      "learning_rate": 3.7037037037037037e-05,
      "loss": 0.3156280517578125,
      "step": 51
    },
    {
      "epoch": 0.5793871866295265,
      "grad_norm": 0.25842663645744324,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.28905943036079407,
      "step": 52
    },
    {
      "epoch": 0.5905292479108635,
      "grad_norm": 0.2600284814834595,
      "learning_rate": 2.962962962962963e-05,
      "loss": 0.2777100205421448,
      "step": 53
    },
    {
      "epoch": 0.6016713091922006,
      "grad_norm": 0.2802374064922333,
      "learning_rate": 2.5925925925925925e-05,
      "loss": 0.3053264021873474,
      "step": 54
    },
    {
      "epoch": 0.6128133704735376,
      "grad_norm": 0.27135026454925537,
      "learning_rate": 2.2222222222222223e-05,
      "loss": 0.33152860403060913,
      "step": 55
    },
    {
      "epoch": 0.6239554317548747,
      "grad_norm": 0.2874375879764557,
      "learning_rate": 1.8518518518518518e-05,
      "loss": 0.3238714039325714,
      "step": 56
    },
    {
      "epoch": 0.6350974930362117,
      "grad_norm": 0.2863210439682007,
      "learning_rate": 1.4814814814814815e-05,
      "loss": 0.34703442454338074,
      "step": 57
    },
    {
      "epoch": 0.6462395543175488,
      "grad_norm": 0.28228458762168884,
      "learning_rate": 1.1111111111111112e-05,
      "loss": 0.35848748683929443,
      "step": 58
    },
    {
      "epoch": 0.6573816155988857,
      "grad_norm": 0.27289554476737976,
      "learning_rate": 7.4074074074074075e-06,
      "loss": 0.34739866852760315,
      "step": 59
    },
    {
      "epoch": 0.6685236768802229,
      "grad_norm": 0.2573246359825134,
      "learning_rate": 3.7037037037037037e-06,
      "loss": 0.26636162400245667,
      "step": 60
    }
  ],
  "logging_steps": 1,
  "max_steps": 60,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.2133020851970048e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
