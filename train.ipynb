{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a4a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs Unsloth, Xformers (Flash Attention), and TRL\n",
    "!uv pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!uv pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a4de73",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -q -U bitsandbytes\n",
    "# Use stable releases instead of git main to avoid breaking changes\n",
    "# !uv pip install -q -U \"transformers>=4.36.0,<4.50.0\"  # Stable version that works with bnb, \n",
    "# !uv pip install transformers -U\n",
    "# changed from 'git+https://github.com/huggingface/transformers.git'\n",
    "!uv pip install -q -U \"peft>=0.7.0\"\n",
    "!uv pip install -q -U \"accelerate>=0.25.0\"\n",
    "!uv pip install -q datasets\n",
    "!uv pip install -q pandas\n",
    "!uv pip install -q tensorboard\n",
    "!uv pip install -q -U \"huggingface-hub>=0.34.0,<1.0\"\n",
    "!uv pip install -q trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6daa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install torch torchvision\n",
    "# !uv pip install \"transformers>=5.0.0rc1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86818ea5",
   "metadata": {},
   "source": [
    "### Create Inference Harness\n",
    "\n",
    "The next two cells are just to create a simple inference harness which we will use to do quick evals whilst we review our checkpoints. They are nothing to do with training the model and we‚Äôll revisit later.\n",
    "\n",
    "---\n",
    "\n",
    "The line `os.environ['TOKENIZERS_PARALLELISM'] = 'false'` is just to stop warnings where HF tokens use multiple CPU cores by default\n",
    "\n",
    "When combined with PyTorch's DataLoader (which also uses multiprocessing), you can get conflicts so we set to `False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a08ae03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Apply CSS to enable text wrapping in code output\n",
    "display(HTML('''\n",
    "<style>\n",
    "  pre {\n",
    "      white-space: pre-wrap;\n",
    "  }\n",
    "</style>\n",
    "'''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab47cb9",
   "metadata": {},
   "source": [
    "Set up the transformers inference API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c7861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(query: str, model, tokenizer) -> str:\n",
    "  device = \"cuda:0\"\n",
    "\n",
    "  # IMPORTANT: Must match training format exactly!\n",
    "  # Training uses \"### Instruction:\" and \"### Response:\", not \"Question/Answer\"\n",
    "  prompt_template = \"\"\"### Instruction:\n",
    "{query}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "  prompt = prompt_template.format(query=query)\n",
    "\n",
    "  encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "  model_inputs = encodeds.to(device)\n",
    "\n",
    "\n",
    "  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "  decoded = tokenizer.batch_decode(generated_ids)\n",
    "  return (decoded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2f45e9",
   "metadata": {},
   "source": [
    "### 3. Model & Tokenizer loading \n",
    "\n",
    "We'll load the model using **QLoRA** quantization to reduce the usage of memory\n",
    "In full fine-tuning:\n",
    "Our optimizer **AdamW** updates every weight matrix in the neural network.\n",
    "\n",
    "\n",
    "We use FastLanguageModel here.\n",
    "\n",
    "## Important: \n",
    "\n",
    "I've set it to load the BF16 Reasoning model in 4-bit mode, which fits perfectly on consumer GPUs while keeping high accuracy.\n",
    "\n",
    "This should change with the instruct fp 8 model i think?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f7d50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# 1. Configuration\n",
    "max_seq_length = 2048\n",
    "dtype = None # Auto-detects your GPU capabilities\n",
    "load_in_4bit = True # This replaces your 'bnb_config'\n",
    "\n",
    "# 2. Load BOTH Model and Tokenizer\n",
    "# Use Unsloth's pre-converted version!\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Ministral-3-3B-Instruct-2512\",  # Match the download name!\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed046041",
   "metadata": {},
   "source": [
    "### 3. Configure LoRA:\n",
    "\n",
    "Unsloth handles the target modules automatically (including the tricky gate_proj, up_proj, etc. that vanilla Peft requires you to list manually)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176ad2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e156dbf",
   "metadata": {},
   "source": [
    "4. Data Loading & Formatting (The \"Junior Accountant\" Logic):\n",
    "\n",
    "This is where we inject your specific \"Junior Accountant\" System Prompt.\n",
    "\n",
    "It maps your refined_data.json to the Mistral chat format automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce9ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Define your custom System Prompt\n",
    "system_prompt = \"\"\"You are an expert accountant using Beancount syntax.\n",
    "Instructions:\n",
    "1. Analyze the transaction and the historical <context>.\n",
    "2. FORMULATE A PLAN inside <plan> tags. Decide the high-level category (Asset, Liability, Income, Expense) and the double-entry logic.\n",
    "3. EXECUTE THE PLAN inside <reasoning> tags. Verify the account name against history and confirm the math balances to zero.\n",
    "4. WRITE THE CODE inside <entry> tags. Use strict Beancount syntax.\n",
    "IMPORTANT: Output ONLY the raw XML.\"\"\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    conversations = []\n",
    "    \n",
    "    # We assume your JSON has 'data.prompt' (input) and 'predictions...text' (output)\n",
    "    # You might need to adjust these keys based on exactly how Label Studio exported the JSON\n",
    "    # This example assumes a flat format: {\"prompt\": \"...\", \"response\": \"...\"}\n",
    "    # If using raw Label Studio export, let me know and I can tweak this extraction!\n",
    "    \n",
    "    for prompt, response in zip(examples['prompt'], examples['response']):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "            {\"role\": \"assistant\", \"content\": response}\n",
    "        ]\n",
    "        \n",
    "        # Apply the chat template (Correctly handles [INST] tags)\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "        conversations.append(text)\n",
    "        \n",
    "    return { \"text\" : conversations }\n",
    "\n",
    "# Load your dataset\n",
    "# Make sure 'refined_data.json' is formatted with 'prompt' and 'response' fields!\n",
    "dataset = load_dataset(\"json\", data_files=\"final_train.json\", split=\"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dcb85c",
   "metadata": {},
   "source": [
    "## Critical Check: JSON Format\n",
    "Label Studio exports JSON in a nested format (inside predictions, result, etc.), but load_dataset usually expects a flat list of {\"prompt\": \"...\", \"response\": \"...\"}.\n",
    "\n",
    "Before running this, run a quick Python script to flatten your refined_data.json into a train.json for Unsloth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6067a5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "input_file = \"final_train.json\"\n",
    "output_file = \"ready_to_train.json\"\n",
    "\n",
    "print(f\"üìñ Reading {input_file}...\")\n",
    "with open(input_file, \"r\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "flat_data = []\n",
    "skipped_count = 0\n",
    "\n",
    "for item in raw_data:\n",
    "    try:\n",
    "        # 1. Extract Prompt\n",
    "        prompt = item['data']['prompt']\n",
    "        \n",
    "        # 2. Extract Response (CRITICAL CHANGE: Look in 'annotations', not 'predictions')\n",
    "        # The Senior Accountant saves the final version in 'annotations'\n",
    "        response_text = item['annotations'][0]['result'][0]['value']['text'][0]\n",
    "        \n",
    "        # 3. AUTO-CLEANUP: Fix the \"Space after Colon\" bug\n",
    "        # Claude wrote \"Assets: Lloyds:Checking\", but Beancount prefers \"Assets:Lloyds:Checking\"\n",
    "        # This regex removes the space after the colon for the 5 root account types\n",
    "        response_text = re.sub(r'(Assets|Liabilities|Expenses|Income|Equity):\\s+', r'\\1:', response_text)\n",
    "        \n",
    "        flat_data.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"response\": response_text\n",
    "        })\n",
    "        \n",
    "    except (KeyError, IndexError) as e:\n",
    "        # This catches any malformed records\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "\n",
    "# 4. Save flattened file\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(flat_data, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Success! Processed {len(flat_data)} records.\")\n",
    "if skipped_count > 0:\n",
    "    print(f\"‚ö†Ô∏è Skipped {skipped_count} malformed records.\")\n",
    "print(f\"üíæ Saved to {output_file} - You are ready to train!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cde9498",
   "metadata": {},
   "source": [
    "### Check where the model is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098082b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check where the model is cached\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "\n",
    "cache_dir = os.path.expanduser(\"~/.cache/huggingface/hub/\")\n",
    "print(f\"Model cache location: {cache_dir}\")\n",
    "print(\"\\nContents:\")\n",
    "if os.path.exists(cache_dir):\n",
    "    for item in os.listdir(cache_dir)[:10]:  # Show first 10 items\n",
    "        print(f\"  - {item}\")\n",
    "else:\n",
    "    print(\"Cache directory not found yet\")\n",
    "\n",
    "# You can also set a custom cache location if you prefer:\n",
    "# os.environ['HF_HOME'] = '/path/to/custom/cache'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc77db5",
   "metadata": {},
   "source": [
    "## Apply QLora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732dd1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from trl import SFTTrainer, SFTConfig  # Changed import\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,\n",
    "    args = SFTConfig(  # Changed from TrainingArguments to SFTConfig\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        # These are now part of SFTConfig\n",
    "        max_seq_length = max_seq_length,\n",
    "        dataset_text_field = \"text\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Start Training\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4381ced9",
   "metadata": {},
   "source": [
    "Quick calculation:\n",
    "\n",
    "700 records\n",
    "Effective batch size = per_device_batch_size (2) √ó gradient_accumulation_steps (4) = 8\n",
    "Steps per epoch = 700 / 8 = ~88 steps\n",
    "So 60 steps = ~0.7 epochs - you haven't even completed one full pass through your data yet!\n",
    "\n",
    "Recommendations:\n",
    "\n",
    "Epochs |\tSteps |\tUse Case |\n",
    "1 |\t~90 |\tMinimum - sees all data once |\n",
    "2-3 |\t~180-270|\tSweet spot for fine-tuning |\n",
    "5+ |\t440+ |\tRisk of overfitting |\n",
    "\n",
    "Since your loss was still decreasing at step 60, you probably have room to train more. I'd suggest trying max_steps = 180 (about 2 epochs) for a good balance.\n",
    "\n",
    "Watch for:\n",
    "\n",
    "‚úÖ Good sign: Loss continues decreasing smoothly\n",
    "‚ö†Ô∏è Overfitting warning: Loss drops very low (<0.1) or starts fluctuating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402268d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training from step 60 to step 180\n",
    "trainer.args.max_steps = 180  # New target\n",
    "\n",
    "# Resume from the last checkpoint\n",
    "trainer_stats = trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5d57da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training from step 60 to step 180\n",
    "trainer.args.max_steps = 270  # New target\n",
    "\n",
    "# Resume from the last checkpoint\n",
    "trainer_stats = trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00db7b43",
   "metadata": {},
   "source": [
    "### LOGIN TO HUB\n",
    "\n",
    "When we push to HuggingFace Hub, it will merge our local QLoRa adaptor with the base model we used to train, on the hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11efb6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Try to login with token from environment variable\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "if hf_token:\n",
    "\tlogin(token=hf_token)\n",
    "\tprint(\"Logged in with HF_TOKEN environment variable\") \n",
    "else:\n",
    "\t# Skip login for local training - you can still train without pushing to hub\n",
    "\tprint(\"No HF_TOKEN found. Proceeding with local training on local GPU...\")\n",
    "\tprint(\"Note: You won't be able to push models to HuggingFace Hub without authentication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52519998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import os\n",
    "device = \"cuda:0\"\n",
    "\n",
    "# 1. CONFIGURATION\n",
    "# Point this to the exact folder on your disk\n",
    "checkpoint_path = \"outputs/checkpoint-180\" \n",
    "repo_name = \"DataImaginations/ministral-3B-Beancount-v1\" # Your Hugging Face repo\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "\n",
    "# 2. LOAD SPECIFIC CHECKPOINT\n",
    "# Unsloth is smart: if you point it to a folder, it loads the base model \n",
    "# AND applies the adapters from that folder automatically.\n",
    "print(f\"üìÇ Loading checkpoint from {checkpoint_path}...\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = checkpoint_path, \n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True, # Keep True for fast loading (Unsloth handles the merge magic)\n",
    ")\n",
    "\n",
    "# 3. MERGE & PUSH\n",
    "# This will de-quantize the base model, merge your checkpoint-180 adapters, \n",
    "# and upload a clean 16-bit model to the Hub.\n",
    "print(f\"üöÄ Merging and pushing to {repo_name}...\")\n",
    "\n",
    "model.push_to_hub_merged(\n",
    "    repo_name,\n",
    "    tokenizer,\n",
    "    save_method = \"merged_16bit\", # options: \"merged_4bit\", \"merged_16bit\"\n",
    "    token = hf_token\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Done! Your Junior Accountant (Checkpoint 180) is live!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b4fe6c",
   "metadata": {},
   "source": [
    "## ALL EDITS BELOW ARE PURELY ME IN PACKAGE HELL AFTER USING A MODEL SO NEW THAT TRANSFORMERS AND UNSLOTH DON'T MATCH\n",
    "\n",
    "### IF YOU'RE USING THIS 1 WEEK+ AFTER 18/12/2025 YOU WON'T NEED THE BELOW (WHICH DIDN'T WORK ANYWAY)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a54c04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip uninstall unsloth unsloth_zoo\n",
    "!uv pip uninstall unsloth unsloth_zoo  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f309ca6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unsloth\n",
    "# This SHOULD fail with ModuleNotFoundError. If it doesn't, manual deletion is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57704ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies FIRST\n",
    "!uv pip install \"transformers==4.46.3\" \"peft==0.13.2\" \"trl==0.8.6\" \"accelerate==0.34.2\" \"huggingface_hub>=0.26.0\" \"bitsandbytes==0.44.1\"\n",
    "# Install Unsloth Stable (PyPI version, NOT git)\n",
    "!uv pip install \"unsloth==2024.11.7\"  # November stable release known to work with TR 4.46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec81e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Uninstall existing packages to prevent conflicts\n",
    "!uv pip uninstall transformers peft trl unsloth accelerate\n",
    "# 2. Install \"Known Good\" compatible versions (Late 2024 Stable Stack)\n",
    "# - Transformers 4.46.3: Stable, works with Unsloth and HF Hub\n",
    "# - PEFT 0.13.2: Compatible with Tr 4.46\n",
    "# - TRL 0.8.6: Compatible with Tr 4.46\n",
    "# - Accelerate 0.34.2: Stable backend\n",
    "!uv pip install \"transformers==4.46.3\" \"peft==0.13.2\" \"trl==0.8.6\" \"accelerate==0.34.2\" \"huggingface_hub>=0.26.0\" \"bitsandbytes==0.44.1\"\n",
    "# 3. Install Unsloth WITHOUT upgrading dependencies\n",
    "# This forces it to use our pinned versions\n",
    "!uv pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" --no-deps\n",
    "# 2. Restart your kernel is REQUIRED after this!\n",
    "print(\"Please restart your kernel now!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d1a3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install \"bitsandbytes==0.44.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcbb74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -q -U bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435fb190",
   "metadata": {},
   "source": [
    "# Push Model to hub!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ed54f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Try to login with token from environment variable\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "if hf_token:\n",
    "\tlogin(token=hf_token)\n",
    "\tprint(\"Logged in with HF_TOKEN environment variable\") \n",
    "else:\n",
    "\t# Skip login for local training - you can still train without pushing to hub\n",
    "\tprint(\"No HF_TOKEN found. Proceeding with local training on local GPU...\")\n",
    "\tprint(\"Note: You won't be able to push models to HuggingFace Hub without authentication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295378f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, MistralConfig\n",
    "try:\n",
    "    AutoConfig.register(\"mistral3\", MistralConfig)\n",
    "    AutoConfig.register(\"ministral3\", MistralConfig)\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5427de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import os\n",
    "device = \"cuda:0\"\n",
    "\n",
    "# 1. CONFIGURATION\n",
    "# Point this to the exact folder on your disk\n",
    "checkpoint_path = \"outputs/checkpoint-180\" \n",
    "repo_name = \"DataImaginations/ministral-3B-Beancount-v1\" # Your Hugging Face repo\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "\n",
    "# 2. LOAD SPECIFIC CHECKPOINT\n",
    "# Unsloth is smart: if you point it to a folder, it loads the base model \n",
    "# AND applies the adapters from that folder automatically.\n",
    "print(f\"üìÇ Loading checkpoint from {checkpoint_path}...\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = checkpoint_path, \n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True, # Keep True for fast loading (Unsloth handles the merge magic)\n",
    ")\n",
    "\n",
    "# 3. MERGE & PUSH\n",
    "# This will de-quantize the base model, merge your checkpoint-180 adapters, \n",
    "# and upload a clean 16-bit model to the Hub.\n",
    "print(f\"üöÄ Merging and pushing to {repo_name}...\")\n",
    "\n",
    "model.push_to_hub_merged(\n",
    "    repo_name,\n",
    "    tokenizer,\n",
    "    save_method = \"merged_16bit\", # options: \"merged_4bit\", \"merged_16bit\"\n",
    "    token = hf_token\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Done! Your Junior Accountant (Checkpoint 180) is live!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4bf6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Completely clean slate\n",
    "!uv pip uninstall transformers unsloth peft trl accelerate bitsandbytes\n",
    "\n",
    "# 2. Install the KNOWN WORKING stack from late 2024\n",
    "!uv pip install \"transformers==4.46.3\" \"peft==0.13.2\" \"trl==0.8.6\" \"accelerate==0.34.2\" \"bitsandbytes==0.44.1\" \"huggingface_hub>=0.26.0\"\n",
    " \n",
    "# 3. Install Unsloth WITHOUT letting it upgrade dependencies\n",
    "!uv pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" --no-deps\n",
    "\n",
    "print(\"‚úÖ Done! RESTART YOUR KERNEL NOW before running any other cells!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88417a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import os\n",
    "\n",
    "checkpoint_path = \"outputs/checkpoint-180\"\n",
    "repo_name = \"DataImaginations/ministral-3B-Beancount-v1\"\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=checkpoint_path,\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "model.push_to_hub_merged(\n",
    "    repo_name,\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\",\n",
    "    token=hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a38324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load base model\n",
    "base_model_name = \"unsloth/Ministral-3-3B-Instruct-2512\"  # Or the Unsloth version\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "# Load and merge adapter\n",
    "model = PeftModel.from_pretrained(model, \"outputs/checkpoint-180\")\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Push to hub\n",
    "model.push_to_hub(\"DataImaginations/ministral-3B-Beancount-v1\")\n",
    "tokenizer.push_to_hub(\"DataImaginations/ministral-3B-Beancount-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49df4d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, reinstall with compatible versions (no downgrade needed)\n",
    "# !uv pip install -q peft transformers huggingface_hub torch\n",
    "\n",
    "# Then run the script\n",
    "%run merge_and_push.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646b16a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Upgrade transformers (restart kernel after this!)\n",
    "!uv pip install -U transformers peft huggingface_hub\n",
    "print(\"‚úÖ Done! RESTART YOUR KERNEL NOW!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e0cd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Run the merge\n",
    "%run merge_and_push.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
