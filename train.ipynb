{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "223a4a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m81 packages\u001b[0m \u001b[2min 661ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2mUninstalled \u001b[1m5 packages\u001b[0m \u001b[2min 63ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m5 packages\u001b[0m \u001b[2min 85ms\u001b[0m\u001b[0m                                \u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1maccelerate\u001b[0m\u001b[2m==0.34.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maccelerate\u001b[0m\u001b[2m==1.12.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mbitsandbytes\u001b[0m\u001b[2m==0.44.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mbitsandbytes\u001b[0m\u001b[2m==0.49.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.20.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.22.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.46.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.57.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtrl\u001b[0m\u001b[2m==0.8.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtrl\u001b[0m\u001b[2m==0.24.0\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m3 packages\u001b[0m \u001b[2min 52ms\u001b[0m\u001b[0m                                          \u001b[0m\n",
      "\u001b[2K   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m xformers\u001b[2m==0.0.26.post1\u001b[0m                                     \n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m xformers\u001b[2m==0.0.26.post1\u001b[0m                             \u001b[1A\n",
      "\u001b[2K  \u001b[31m√ó\u001b[0m Failed to build `xformers==0.0.26.post1`6.post1\u001b[0m                                              \n",
      "\u001b[31m  ‚îú‚îÄ‚ñ∂ \u001b[0mThe build backend returned an error\n",
      "\u001b[31m  ‚ï∞‚îÄ‚ñ∂ \u001b[0mCall to `setuptools.build_meta:__legacy__.build_wheel` failed (exit\n",
      "\u001b[31m      \u001b[0mstatus: 1)\n",
      "\n",
      "\u001b[31m      \u001b[0m\u001b[31m[stderr]\u001b[39m\n",
      "\u001b[31m      \u001b[0mTraceback (most recent call last):\n",
      "\u001b[31m      \u001b[0m  File \"<string>\", line 14, in <module>\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\"/home/david-barnes/.cache/uv/builds-v0/.tmpE8h8au/lib/python3.12/site-packages/setuptools/build_meta.py\",\n",
      "\u001b[31m      \u001b[0mline 331, in get_requires_for_build_wheel\n",
      "\u001b[31m      \u001b[0m    return self._get_build_requires(config_settings, requirements=[])\n",
      "\u001b[31m      \u001b[0m           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\"/home/david-barnes/.cache/uv/builds-v0/.tmpE8h8au/lib/python3.12/site-packages/setuptools/build_meta.py\",\n",
      "\u001b[31m      \u001b[0mline 301, in _get_build_requires\n",
      "\u001b[31m      \u001b[0m    self.run_setup()\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\"/home/david-barnes/.cache/uv/builds-v0/.tmpE8h8au/lib/python3.12/site-packages/setuptools/build_meta.py\",\n",
      "\u001b[31m      \u001b[0mline 512, in run_setup\n",
      "\u001b[31m      \u001b[0m    super().run_setup(setup_script=setup_script)\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\"/home/david-barnes/.cache/uv/builds-v0/.tmpE8h8au/lib/python3.12/site-packages/setuptools/build_meta.py\",\n",
      "\u001b[31m      \u001b[0mline 317, in run_setup\n",
      "\u001b[31m      \u001b[0m    exec(code, locals())\n",
      "\u001b[31m      \u001b[0m  File \"<string>\", line 23, in <module>\n",
      "\u001b[31m      \u001b[0mModuleNotFoundError: No module named 'torch'\n",
      "\n",
      "\u001b[31m      \u001b[0m\u001b[36m\u001b[1mhint\u001b[0m\u001b[39m\u001b[1m:\u001b[0m This error likely indicates that `\u001b[36mxformers@0.0.26.post1\u001b[39m`\n",
      "\u001b[31m      \u001b[0mdepends on `\u001b[36mtorch\u001b[39m`, but doesn't declare it as a build dependency. If\n",
      "\u001b[31m      \u001b[0m`\u001b[36mxformers\u001b[39m` is a first-party package, consider adding `\u001b[36mtorch\u001b[39m` to its\n",
      "\u001b[31m      \u001b[0m`\u001b[32mbuild-system.requires\u001b[39m`. Otherwise, `\u001b[32muv pip install torch\u001b[39m` into the\n",
      "\u001b[31m      \u001b[0menvironment and re-run with `\u001b[32m--no-build-isolation\u001b[39m`.\n"
     ]
    }
   ],
   "source": [
    "# Installs Unsloth, Xformers (Flash Attention), and TRL\n",
    "!uv pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!uv pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6a4de73",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -q -U bitsandbytes\n",
    "# Use stable releases instead of git main to avoid breaking changes\n",
    "# !uv pip install -q -U \"transformers>=4.36.0,<4.50.0\"  # Stable version that works with bnb, \n",
    "# !uv pip install transformers -U\n",
    "# changed from 'git+https://github.com/huggingface/transformers.git'\n",
    "!uv pip install -q -U \"peft>=0.7.0\"\n",
    "!uv pip install -q -U \"accelerate>=0.25.0\"\n",
    "!uv pip install -q datasets\n",
    "!uv pip install -q pandas\n",
    "!uv pip install -q tensorboard\n",
    "!uv pip install -q -U \"huggingface-hub>=0.34.0,<1.0\"\n",
    "!uv pip install -q trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e6daa9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m2 packages\u001b[0m \u001b[2min 6ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install torch torchvision\n",
    "# !uv pip install \"transformers>=5.0.0rc1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86818ea5",
   "metadata": {},
   "source": [
    "### Create Inference Harness\n",
    "\n",
    "The next two cells are just to create a simple inference harness which we will use to do quick evals whilst we review our checkpoints. They are nothing to do with training the model and we‚Äôll revisit later.\n",
    "\n",
    "---\n",
    "\n",
    "The line `os.environ['TOKENIZERS_PARALLELISM'] = 'false'` is just to stop warnings where HF tokens use multiple CPU cores by default\n",
    "\n",
    "When combined with PyTorch's DataLoader (which also uses multiprocessing), you can get conflicts so we set to `False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a08ae03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  pre {\n",
       "      white-space: pre-wrap;\n",
       "  }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Apply CSS to enable text wrapping in code output\n",
    "display(HTML('''\n",
    "<style>\n",
    "  pre {\n",
    "      white-space: pre-wrap;\n",
    "  }\n",
    "</style>\n",
    "'''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab47cb9",
   "metadata": {},
   "source": [
    "Set up the transformers inference API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6c7861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(query: str, model, tokenizer) -> str:\n",
    "  device = \"cuda:0\"\n",
    "\n",
    "  # IMPORTANT: Must match training format exactly!\n",
    "  # Training uses \"### Instruction:\" and \"### Response:\", not \"Question/Answer\"\n",
    "  prompt_template = \"\"\"### Instruction:\n",
    "{query}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "  prompt = prompt_template.format(query=query)\n",
    "\n",
    "  encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "  model_inputs = encodeds.to(device)\n",
    "\n",
    "\n",
    "  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "  decoded = tokenizer.batch_decode(generated_ids)\n",
    "  return (decoded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2f45e9",
   "metadata": {},
   "source": [
    "### 3. Model & Tokenizer loading \n",
    "\n",
    "We'll load the model using **QLoRA** quantization to reduce the usage of memory\n",
    "In full fine-tuning:\n",
    "Our optimizer **AdamW** updates every weight matrix in the neural network.\n",
    "\n",
    "\n",
    "We use FastLanguageModel here.\n",
    "\n",
    "## Important: \n",
    "\n",
    "I've set it to load the BF16 Reasoning model in 4-bit mode, which fits perfectly on consumer GPUs while keeping high accuracy.\n",
    "\n",
    "This should change with the instruct fp 8 model i think?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48f7d50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12070/3982636233.py:1: UserWarning: WARNING: Unsloth should be imported before [transformers, peft] to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "cannot import name 'is_soundfile_available' from 'transformers.utils' (/home/david-barnes/Programs/beancount_projects/.venv/lib/python3.12/site-packages/transformers/utils/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/beancount_projects/.venv/lib/python3.12/site-packages/unsloth_zoo/temporary_patches/utils.py:107\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprocessing_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Unpack\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \\\n\u001b[32m    109\u001b[39m         \u001b[38;5;28mtype\u001b[39m(Unpack) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(t_Unpack), \\\n\u001b[32m    110\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnsloth: Unpack type changed! Please file a bug report asap!\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/beancount_projects/.venv/lib/python3.12/site-packages/transformers/processing_utils.py:34\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EntryNotFoundError\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01maudio_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioInput, load_audio\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdynamic_module_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m custom_object_save\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/beancount_projects/.venv/lib/python3.12/site-packages/transformers/audio_utils.py:35\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpackaging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     36\u001b[39m     is_librosa_available,\n\u001b[32m     37\u001b[39m     is_numpy_array,\n\u001b[32m     38\u001b[39m     is_soundfile_available,\n\u001b[32m     39\u001b[39m     is_torch_tensor,\n\u001b[32m     40\u001b[39m     is_torchcodec_available,\n\u001b[32m     41\u001b[39m     requires_backends,\n\u001b[32m     42\u001b[39m )\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_soundfile_available():\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'is_soundfile_available' from 'transformers.utils' (/home/david-barnes/Programs/beancount_projects/.venv/lib/python3.12/site-packages/transformers/utils/__init__.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 1. Configuration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/beancount_projects/.venv/lib/python3.12/site-packages/unsloth/__init__.py:92\u001b[39m\n\u001b[32m     80\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m     81\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mUnsloth: Please update Unsloth and Unsloth-Zoo to the latest version!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     82\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mDo this via `pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     83\u001b[39m         )\n\u001b[32m     84\u001b[39m         \u001b[38;5;66;03m# if os.environ.get(\"UNSLOTH_DISABLE_AUTO_UPDATES\", \"0\") == \"0\":\u001b[39;00m\n\u001b[32m     85\u001b[39m         \u001b[38;5;66;03m#     try:\u001b[39;00m\n\u001b[32m     86\u001b[39m         \u001b[38;5;66;03m#         os.system(\"pip install --upgrade --no-cache-dir --no-deps unsloth_zoo\")\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     90\u001b[39m         \u001b[38;5;66;03m#         except:\u001b[39;00m\n\u001b[32m     91\u001b[39m         \u001b[38;5;66;03m#             raise ImportError(\"Unsloth: Please update unsloth_zoo via `pip install --upgrade --no-cache-dir --no-deps unsloth_zoo`\")\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth_zoo\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m PackageNotFoundError:\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     95\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsloth: Please install unsloth_zoo via `pip install unsloth_zoo` then retry!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     96\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/beancount_projects/.venv/lib/python3.12/site-packages/unsloth_zoo/__init__.py:194\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;66;03m# Log Unsloth-Zoo Utilities\u001b[39;00m\n\u001b[32m    192\u001b[39m os.environ[\u001b[33m\"\u001b[39m\u001b[33mUNSLOTH_ZOO_IS_PRESENT\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtemporary_patches\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    195\u001b[39m     encode_conversations_with_harmony,\n\u001b[32m    196\u001b[39m )\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrl_environments\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    198\u001b[39m     check_python_modules,\n\u001b[32m    199\u001b[39m     create_locked_down_function,\n\u001b[32m   (...)\u001b[39m\u001b[32m    203\u001b[39m     launch_openenv,\n\u001b[32m    204\u001b[39m )\n\u001b[32m    206\u001b[39m \u001b[38;5;66;03m# Top some pydantic warnings\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/beancount_projects/.venv/lib/python3.12/site-packages/unsloth_zoo/temporary_patches/__init__.py:19\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Unsloth Zoo - Utilities for Unsloth\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Copyright 2023-present Daniel Han-Chen, Michael Han-Chen & the Unsloth team. All rights reserved.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# You should have received a copy of the GNU Lesser General Public License\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# along with this program.  If not, see <https://www.gnu.org/licenses/>.\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgemma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmisc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgemma3n\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/beancount_projects/.venv/lib/python3.12/site-packages/unsloth_zoo/temporary_patches/gemma.py:22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TEMPORARY_PATCHES, torch_compile\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     23\u001b[39m     patch_function,\n\u001b[32m     24\u001b[39m     process_output_options,\n\u001b[32m     25\u001b[39m     KWARGS_TYPE,\n\u001b[32m     26\u001b[39m     raise_error,\n\u001b[32m     27\u001b[39m     ImageInput,\n\u001b[32m     28\u001b[39m     PreTokenizedInput,\n\u001b[32m     29\u001b[39m     TextInput,\n\u001b[32m     30\u001b[39m     Cache,\n\u001b[32m     31\u001b[39m     StaticCache,\n\u001b[32m     32\u001b[39m     HybridCache,\n\u001b[32m     33\u001b[39m     Unpack,\n\u001b[32m     34\u001b[39m     patch_function_past_key_values,\n\u001b[32m     35\u001b[39m     dedent,\n\u001b[32m     36\u001b[39m )\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minspect\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpatch_Gemma3Processor\u001b[39m():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/beancount_projects/.venv/lib/python3.12/site-packages/unsloth_zoo/temporary_patches/utils.py:122\u001b[39m\n\u001b[32m    118\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    119\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m***** Please update and reinstall torchvision - it broke! `pip install --upgrade --force-reinstall --no-cache-dir torchvision` *****\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    120\u001b[39m         )\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mUnpack\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(e)\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    124\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsloth: Unpack has been moved! Other error = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\\\n\u001b[32m    125\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease file a bug report asap!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    126\u001b[39m     )\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mException\u001b[39m: cannot import name 'is_soundfile_available' from 'transformers.utils' (/home/david-barnes/Programs/beancount_projects/.venv/lib/python3.12/site-packages/transformers/utils/__init__.py)"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# 1. Configuration\n",
    "max_seq_length = 2048\n",
    "dtype = None # Auto-detects your GPU capabilities\n",
    "load_in_4bit = True # This replaces your 'bnb_config'\n",
    "\n",
    "# 2. Load BOTH Model and Tokenizer\n",
    "# Use Unsloth's pre-converted version!\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Ministral-3-3B-Instruct-2512\",  # Match the download name!\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed046041",
   "metadata": {},
   "source": [
    "### 3. Configure LoRA:\n",
    "\n",
    "Unsloth handles the target modules automatically (including the tricky gate_proj, up_proj, etc. that vanilla Peft requires you to list manually)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176ad2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e156dbf",
   "metadata": {},
   "source": [
    "4. Data Loading & Formatting (The \"Junior Accountant\" Logic):\n",
    "\n",
    "This is where we inject your specific \"Junior Accountant\" System Prompt.\n",
    "\n",
    "It maps your refined_data.json to the Mistral chat format automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce9ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Define your custom System Prompt\n",
    "system_prompt = \"\"\"You are an expert accountant using Beancount syntax.\n",
    "Instructions:\n",
    "1. Analyze the transaction and the historical <context>.\n",
    "2. FORMULATE A PLAN inside <plan> tags. Decide the high-level category (Asset, Liability, Income, Expense) and the double-entry logic.\n",
    "3. EXECUTE THE PLAN inside <reasoning> tags. Verify the account name against history and confirm the math balances to zero.\n",
    "4. WRITE THE CODE inside <entry> tags. Use strict Beancount syntax.\n",
    "IMPORTANT: Output ONLY the raw XML.\"\"\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    conversations = []\n",
    "    \n",
    "    # We assume your JSON has 'data.prompt' (input) and 'predictions...text' (output)\n",
    "    # You might need to adjust these keys based on exactly how Label Studio exported the JSON\n",
    "    # This example assumes a flat format: {\"prompt\": \"...\", \"response\": \"...\"}\n",
    "    # If using raw Label Studio export, let me know and I can tweak this extraction!\n",
    "    \n",
    "    for prompt, response in zip(examples['prompt'], examples['response']):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "            {\"role\": \"assistant\", \"content\": response}\n",
    "        ]\n",
    "        \n",
    "        # Apply the chat template (Correctly handles [INST] tags)\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "        conversations.append(text)\n",
    "        \n",
    "    return { \"text\" : conversations }\n",
    "\n",
    "# Load your dataset\n",
    "# Make sure 'refined_data.json' is formatted with 'prompt' and 'response' fields!\n",
    "dataset = load_dataset(\"json\", data_files=\"final_train.json\", split=\"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dcb85c",
   "metadata": {},
   "source": [
    "## Critical Check: JSON Format\n",
    "Label Studio exports JSON in a nested format (inside predictions, result, etc.), but load_dataset usually expects a flat list of {\"prompt\": \"...\", \"response\": \"...\"}.\n",
    "\n",
    "Before running this, run a quick Python script to flatten your refined_data.json into a train.json for Unsloth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6067a5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "input_file = \"final_train.json\"\n",
    "output_file = \"ready_to_train.json\"\n",
    "\n",
    "print(f\"üìñ Reading {input_file}...\")\n",
    "with open(input_file, \"r\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "flat_data = []\n",
    "skipped_count = 0\n",
    "\n",
    "for item in raw_data:\n",
    "    try:\n",
    "        # 1. Extract Prompt\n",
    "        prompt = item['data']['prompt']\n",
    "        \n",
    "        # 2. Extract Response (CRITICAL CHANGE: Look in 'annotations', not 'predictions')\n",
    "        # The Senior Accountant saves the final version in 'annotations'\n",
    "        response_text = item['annotations'][0]['result'][0]['value']['text'][0]\n",
    "        \n",
    "        # 3. AUTO-CLEANUP: Fix the \"Space after Colon\" bug\n",
    "        # Claude wrote \"Assets: Lloyds:Checking\", but Beancount prefers \"Assets:Lloyds:Checking\"\n",
    "        # This regex removes the space after the colon for the 5 root account types\n",
    "        response_text = re.sub(r'(Assets|Liabilities|Expenses|Income|Equity):\\s+', r'\\1:', response_text)\n",
    "        \n",
    "        flat_data.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"response\": response_text\n",
    "        })\n",
    "        \n",
    "    except (KeyError, IndexError) as e:\n",
    "        # This catches any malformed records\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "\n",
    "# 4. Save flattened file\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(flat_data, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Success! Processed {len(flat_data)} records.\")\n",
    "if skipped_count > 0:\n",
    "    print(f\"‚ö†Ô∏è Skipped {skipped_count} malformed records.\")\n",
    "print(f\"üíæ Saved to {output_file} - You are ready to train!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cde9498",
   "metadata": {},
   "source": [
    "### Check where the model is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098082b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check where the model is cached\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "\n",
    "cache_dir = os.path.expanduser(\"~/.cache/huggingface/hub/\")\n",
    "print(f\"Model cache location: {cache_dir}\")\n",
    "print(\"\\nContents:\")\n",
    "if os.path.exists(cache_dir):\n",
    "    for item in os.listdir(cache_dir)[:10]:  # Show first 10 items\n",
    "        print(f\"  - {item}\")\n",
    "else:\n",
    "    print(\"Cache directory not found yet\")\n",
    "\n",
    "# You can also set a custom cache location if you prefer:\n",
    "# os.environ['HF_HOME'] = '/path/to/custom/cache'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc77db5",
   "metadata": {},
   "source": [
    "## Apply QLora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732dd1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from trl import SFTTrainer, SFTConfig  # Changed import\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,\n",
    "    args = SFTConfig(  # Changed from TrainingArguments to SFTConfig\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        # These are now part of SFTConfig\n",
    "        max_seq_length = max_seq_length,\n",
    "        dataset_text_field = \"text\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Start Training\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4381ced9",
   "metadata": {},
   "source": [
    "Quick calculation:\n",
    "\n",
    "700 records\n",
    "Effective batch size = per_device_batch_size (2) √ó gradient_accumulation_steps (4) = 8\n",
    "Steps per epoch = 700 / 8 = ~88 steps\n",
    "So 60 steps = ~0.7 epochs - you haven't even completed one full pass through your data yet!\n",
    "\n",
    "Recommendations:\n",
    "\n",
    "Epochs |\tSteps |\tUse Case |\n",
    "1 |\t~90 |\tMinimum - sees all data once |\n",
    "2-3 |\t~180-270|\tSweet spot for fine-tuning |\n",
    "5+ |\t440+ |\tRisk of overfitting |\n",
    "\n",
    "Since your loss was still decreasing at step 60, you probably have room to train more. I'd suggest trying max_steps = 180 (about 2 epochs) for a good balance.\n",
    "\n",
    "Watch for:\n",
    "\n",
    "‚úÖ Good sign: Loss continues decreasing smoothly\n",
    "‚ö†Ô∏è Overfitting warning: Loss drops very low (<0.1) or starts fluctuating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402268d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training from step 60 to step 180\n",
    "trainer.args.max_steps = 180  # New target\n",
    "\n",
    "# Resume from the last checkpoint\n",
    "trainer_stats = trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5d57da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training from step 60 to step 180\n",
    "trainer.args.max_steps = 270  # New target\n",
    "\n",
    "# Resume from the last checkpoint\n",
    "trainer_stats = trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b4fe6c",
   "metadata": {},
   "source": [
    "### Login to Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a54c04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip uninstall unsloth unsloth_zoo\n",
    "!uv pip uninstall unsloth unsloth_zoo  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f309ca6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unsloth\n",
    "# This SHOULD fail with ModuleNotFoundError. If it doesn't, manual deletion is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57704ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies FIRST\n",
    "!uv pip install \"transformers==4.46.3\" \"peft==0.13.2\" \"trl==0.8.6\" \"accelerate==0.34.2\" \"huggingface_hub>=0.26.0\" \"bitsandbytes==0.44.1\"\n",
    "# Install Unsloth Stable (PyPI version, NOT git)\n",
    "!uv pip install \"unsloth==2024.11.7\"  # November stable release known to work with TR 4.46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec81e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Uninstall existing packages to prevent conflicts\n",
    "!uv pip uninstall transformers peft trl unsloth accelerate\n",
    "# 2. Install \"Known Good\" compatible versions (Late 2024 Stable Stack)\n",
    "# - Transformers 4.46.3: Stable, works with Unsloth and HF Hub\n",
    "# - PEFT 0.13.2: Compatible with Tr 4.46\n",
    "# - TRL 0.8.6: Compatible with Tr 4.46\n",
    "# - Accelerate 0.34.2: Stable backend\n",
    "!uv pip install \"transformers==4.46.3\" \"peft==0.13.2\" \"trl==0.8.6\" \"accelerate==0.34.2\" \"huggingface_hub>=0.26.0\" \"bitsandbytes==0.44.1\"\n",
    "# 3. Install Unsloth WITHOUT upgrading dependencies\n",
    "# This forces it to use our pinned versions\n",
    "!uv pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" --no-deps\n",
    "# 2. Restart your kernel is REQUIRED after this!\n",
    "print(\"Please restart your kernel now!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d1a3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install \"bitsandbytes==0.44.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcbb74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -q -U bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435fb190",
   "metadata": {},
   "source": [
    "# Push Model to hub!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92ed54f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david-barnes/Programs/beancount_projects/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in with HF_TOKEN environment variable\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Try to login with token from environment variable\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "if hf_token:\n",
    "\tlogin(token=hf_token)\n",
    "\tprint(\"Logged in with HF_TOKEN environment variable\") \n",
    "else:\n",
    "\t# Skip login for local training - you can still train without pushing to hub\n",
    "\tprint(\"No HF_TOKEN found. Proceeding with local training on local GPU...\")\n",
    "\tprint(\"Note: You won't be able to push models to HuggingFace Hub without authentication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295378f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, MistralConfig\n",
    "try:\n",
    "    AutoConfig.register(\"mistral3\", MistralConfig)\n",
    "    AutoConfig.register(\"ministral3\", MistralConfig)\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5427de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import os\n",
    "device = \"cuda:0\"\n",
    "\n",
    "# 1. CONFIGURATION\n",
    "# Point this to the exact folder on your disk\n",
    "checkpoint_path = \"outputs/checkpoint-180\" \n",
    "repo_name = \"DataImaginations/ministral-3B-Beancount-v1\" # Your Hugging Face repo\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "\n",
    "# 2. LOAD SPECIFIC CHECKPOINT\n",
    "# Unsloth is smart: if you point it to a folder, it loads the base model \n",
    "# AND applies the adapters from that folder automatically.\n",
    "print(f\"üìÇ Loading checkpoint from {checkpoint_path}...\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = checkpoint_path, \n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True, # Keep True for fast loading (Unsloth handles the merge magic)\n",
    ")\n",
    "\n",
    "# 3. MERGE & PUSH\n",
    "# This will de-quantize the base model, merge your checkpoint-180 adapters, \n",
    "# and upload a clean 16-bit model to the Hub.\n",
    "print(f\"üöÄ Merging and pushing to {repo_name}...\")\n",
    "\n",
    "model.push_to_hub_merged(\n",
    "    repo_name,\n",
    "    tokenizer,\n",
    "    save_method = \"merged_16bit\", # options: \"merged_4bit\", \"merged_16bit\"\n",
    "    token = hf_token\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Done! Your Junior Accountant (Checkpoint 180) is live!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4bf6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Completely clean slate\n",
    "!uv pip uninstall transformers unsloth peft trl accelerate bitsandbytes\n",
    "\n",
    "# 2. Install the KNOWN WORKING stack from late 2024\n",
    "!uv pip install \"transformers==4.46.3\" \"peft==0.13.2\" \"trl==0.8.6\" \"accelerate==0.34.2\" \"bitsandbytes==0.44.1\" \"huggingface_hub>=0.26.0\"\n",
    " \n",
    "# 3. Install Unsloth WITHOUT letting it upgrade dependencies\n",
    "!uv pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" --no-deps\n",
    "\n",
    "print(\"‚úÖ Done! RESTART YOUR KERNEL NOW before running any other cells!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88417a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[unsloth_zoo.log|ERROR]Unsloth: KwargsForCausalLM cannot be inherited from TransformersKwargs since it's of type = <class 'type'>\n",
      "[bitsandbytes.cextension|WARNING]Could not find the bitsandbytes CUDA binary at PosixPath('/home/david-barnes/Programs/beancount_projects/.venv/lib/python3.12/site-packages/bitsandbytes/libbitsandbytes_cuda128.so')\n",
      "[bitsandbytes.cextension|WARNING]The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "/home/david-barnes/Programs/beancount_projects/.venv/lib/python3.12/site-packages/unsloth/__init__.py:209: UserWarning: Unsloth: Running `ldconfig /usr/lib64-nvidia` to link CUDA.\n",
      "  warnings.warn(\"Unsloth: Running `ldconfig /usr/lib64-nvidia` to link CUDA.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: `bitsandbytes` is not installed - 4bit QLoRA unallowed, but 16bit and full finetuning works!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sbin/ldconfig.real: Can't create temporary cache file /etc/ld.so.cache~: Permission denied\n",
      "/home/david-barnes/Programs/beancount_projects/.venv/lib/python3.12/site-packages/unsloth/__init__.py:252: UserWarning: Unsloth: CUDA is not linked properly.\n",
      "Try running `python -m bitsandbytes` then `python -m xformers.info`\n",
      "We tried running `ldconfig /usr/lib64-nvidia` ourselves, but it didn't work.\n",
      "You need to run in your terminal `sudo ldconfig /usr/lib64-nvidia` yourself, then import Unsloth.\n",
      "Also try `sudo ldconfig /usr/local/cuda-xx.x` - find the latest cuda version.\n",
      "Unsloth will still run for now, but maybe it might crash - let's hope it works!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'triton.ops'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      4\u001b[39m checkpoint_path = \u001b[33m\"\u001b[39m\u001b[33moutputs/checkpoint-180\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/beancount_projects/.venv/lib/python3.12/site-packages/unsloth/__init__.py:270\u001b[39m\n\u001b[32m    267\u001b[39m     \u001b[38;5;66;03m# TODO: check triton for intel installed properly.\u001b[39;00m\n\u001b[32m    268\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01msave\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/beancount_projects/.venv/lib/python3.12/site-packages/unsloth/models/__init__.py:15\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllama\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLlamaModel\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel, FastVisionModel, FastTextModel, FastModel\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmistral\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastMistralModel\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/beancount_projects/.venv/lib/python3.12/site-packages/unsloth/models/llama.py:21\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfunctools\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optional, Tuple, List, Union\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m patch_unsloth_smart_gradient_checkpointing\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__, importlib_version\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/beancount_projects/.venv/lib/python3.12/site-packages/unsloth/models/_utils.py:110\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth_zoo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenizer_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    102\u001b[39m     patch_tokenizer \u001b[38;5;28;01mas\u001b[39;00m _patch_tokenizer,\n\u001b[32m    103\u001b[39m )\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth_zoo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrl_environments\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    105\u001b[39m     check_python_modules,\n\u001b[32m    106\u001b[39m     create_locked_down_function,\n\u001b[32m    107\u001b[39m     execute_with_time_limit,\n\u001b[32m    108\u001b[39m     Benchmarker,\n\u001b[32m    109\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth_zoo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpatching_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    111\u001b[39m     patch_compiling_bitsandbytes,\n\u001b[32m    112\u001b[39m     patch_layernorm,\n\u001b[32m    113\u001b[39m     patch_torch_compile,\n\u001b[32m    114\u001b[39m     patch_model_and_tokenizer,\n\u001b[32m    115\u001b[39m     patch_compiled_autograd,\n\u001b[32m    116\u001b[39m )\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth_zoo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgradient_checkpointing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    118\u001b[39m     Unsloth_Offloaded_Gradient_Checkpointer,\n\u001b[32m    119\u001b[39m     unsloth_offloaded_gradient_checkpoint,\n\u001b[32m   (...)\u001b[39m\u001b[32m    127\u001b[39m     unpatch_unsloth_smart_gradient_checkpointing,\n\u001b[32m    128\u001b[39m )\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth_zoo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloss_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    130\u001b[39m     HAS_CUT_CROSS_ENTROPY,\n\u001b[32m    131\u001b[39m     fused_linear_cross_entropy,\n\u001b[32m    132\u001b[39m     _unsloth_get_batch_samples,\n\u001b[32m    133\u001b[39m     unsloth_fused_ce_loss,\n\u001b[32m    134\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/beancount_projects/.venv/lib/python3.12/site-packages/unsloth_zoo/patching_utils.py:677\u001b[39m\n\u001b[32m    675\u001b[39m \u001b[38;5;66;03m# Patch for dynamic 4bit quantization\u001b[39;00m\n\u001b[32m    676\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minspect\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbitsandbytes\u001b[39;00m\n\u001b[32m    678\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformers.integrations.bitsandbytes, \u001b[33m\"\u001b[39m\u001b[33m_replace_with_bnb_linear\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    679\u001b[39m     (transformers.integrations.bitsandbytes._replace_with_bnb_linear.\u001b[34m__name__\u001b[39m != \u001b[33m\"\u001b[39m\u001b[33m_unsloth_replace_with_bnb_linear\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    680\u001b[39m \n\u001b[32m    681\u001b[39m     \u001b[38;5;66;03m# All Unsloth Zoo code licensed under LGPLv3\u001b[39;00m\n\u001b[32m    682\u001b[39m     source = inspect.getsource(transformers.integrations.bitsandbytes._replace_with_bnb_linear)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/beancount_projects/.venv/lib/python3.12/site-packages/transformers/integrations/bitsandbytes.py:21\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     10\u001b[39m     get_available_devices,\n\u001b[32m     11\u001b[39m     is_accelerate_available,\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     logging,\n\u001b[32m     17\u001b[39m )\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_bitsandbytes_available():\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbitsandbytes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbnb\u001b[39;00m\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/beancount_projects/.venv/lib/python3.12/site-packages/bitsandbytes/__init__.py:15\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m research, utils\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautograd\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_functions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      8\u001b[39m     MatmulLtState,\n\u001b[32m      9\u001b[39m     bmm_cublas,\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m     mm_cublas,\n\u001b[32m     14\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m modules\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m adam\n\u001b[32m     18\u001b[39m __pdoc__ = {\n\u001b[32m     19\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlibbitsandbytes\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     20\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moptim.optimizer.Optimizer8bit\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     21\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moptim.optimizer.MockArgs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     22\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/beancount_projects/.venv/lib/python3.12/site-packages/bitsandbytes/nn/__init__.py:21\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# This source code is licensed under the MIT license found in the\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodules\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      6\u001b[39m     Embedding,\n\u001b[32m      7\u001b[39m     Embedding4bit,\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m     SwitchBackLinearBnb,\n\u001b[32m     20\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtriton_based_modules\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     22\u001b[39m     StandardLinear,\n\u001b[32m     23\u001b[39m     SwitchBackLinear,\n\u001b[32m     24\u001b[39m     SwitchBackLinearGlobal,\n\u001b[32m     25\u001b[39m     SwitchBackLinearVectorwise,\n\u001b[32m     26\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/beancount_projects/.venv/lib/python3.12/site-packages/bitsandbytes/nn/triton_based_modules.py:7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbitsandbytes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtriton\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdequantize_rowwise\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dequantize_rowwise\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbitsandbytes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtriton\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mint8_matmul_mixed_dequantize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      8\u001b[39m     int8_matmul_mixed_dequantize,\n\u001b[32m      9\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbitsandbytes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtriton\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mint8_matmul_rowwise_dequantize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     11\u001b[39m     int8_matmul_rowwise_dequantize,\n\u001b[32m     12\u001b[39m )\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbitsandbytes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtriton\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquantize_columnwise_and_transpose\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     14\u001b[39m     quantize_columnwise_and_transpose,\n\u001b[32m     15\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/beancount_projects/.venv/lib/python3.12/site-packages/bitsandbytes/triton/int8_matmul_mixed_dequantize.py:12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtriton\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtriton\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlanguage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtl\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtriton\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmatmul_perf_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m early_config_prune, estimate_matmul_time\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# This is a matmul kernel based on triton.ops.matmul\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# It is modified to support rowwise quantized input and global quantized weight\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# It's purpose is fused matmul then dequantize\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# It does support bias.\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minit_to_zero\u001b[39m(name):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'triton.ops'"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import os\n",
    "\n",
    "checkpoint_path = \"outputs/checkpoint-180\"\n",
    "repo_name = \"DataImaginations/ministral-3B-Beancount-v1\"\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=checkpoint_path,\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "model.push_to_hub_merged(\n",
    "    repo_name,\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\",\n",
    "    token=hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a38324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load base model\n",
    "base_model_name = \"unsloth/Ministral-3-3B-Instruct-2512\"  # Or the Unsloth version\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "# Load and merge adapter\n",
    "model = PeftModel.from_pretrained(model, \"outputs/checkpoint-180\")\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Push to hub\n",
    "model.push_to_hub(\"DataImaginations/ministral-3B-Beancount-v1\")\n",
    "tokenizer.push_to_hub(\"DataImaginations/ministral-3B-Beancount-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49df4d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Logged in with HF_TOKEN\n",
      "üöÄ Uploading adapter from outputs/checkpoint-180 to david-barnes/ministral-3B-Beancount-lora-v1...\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "(Request ID: Root=1-6942c6a2-163359762956f17153375d1c;4ede74df-bb54-44f7-809f-0faabb428338)\n\n403 Forbidden: You don't have the rights to create a model under the namespace \"david-barnes\".\nCannot access content at: https://huggingface.co/api/repos/create.\nMake sure your token has the correct permissions.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/beancount_projects/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:402\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/beancount_projects/.venv/lib/python3.12/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 403 Client Error: Forbidden for url: https://huggingface.co/api/repos/create",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/beancount_projects/merge_and_push.py:24\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müöÄ Uploading adapter from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHECKPOINT_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mREPO_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m api = HfApi()\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mREPO_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m api.upload_folder(\n\u001b[32m     26\u001b[39m     folder_path=CHECKPOINT_PATH,\n\u001b[32m     27\u001b[39m     repo_id=REPO_NAME,\n\u001b[32m     28\u001b[39m     token=hf_token,\n\u001b[32m     29\u001b[39m )\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[33m‚úÖ Done! Your LoRA adapter is live at: https://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mREPO_NAME\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     33\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     40\u001b[39m \u001b[33m    model = PeftModel.from_pretrained(base_model, \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mREPO_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[33m\"\"\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/beancount_projects/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/beancount_projects/.venv/lib/python3.12/site-packages/huggingface_hub/hf_api.py:3773\u001b[39m, in \u001b[36mHfApi.create_repo\u001b[39m\u001b[34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[39m\n\u001b[32m   3771\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m RepoUrl(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.endpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   3772\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m HfHubHTTPError:\n\u001b[32m-> \u001b[39m\u001b[32m3773\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m   3774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3775\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/beancount_projects/.venv/lib/python3.12/site-packages/huggingface_hub/hf_api.py:3760\u001b[39m, in \u001b[36mHfApi.create_repo\u001b[39m\u001b[34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[39m\n\u001b[32m   3757\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   3759\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3760\u001b[39m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3761\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m   3762\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exist_ok \u001b[38;5;129;01mand\u001b[39;00m err.response.status_code == \u001b[32m409\u001b[39m:\n\u001b[32m   3763\u001b[39m         \u001b[38;5;66;03m# Repo already exists and `exist_ok=True`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/beancount_projects/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:466\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    460\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m response.status_code == \u001b[32m403\u001b[39m:\n\u001b[32m    461\u001b[39m     message = (\n\u001b[32m    462\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Forbidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    463\u001b[39m         + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCannot access content at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    464\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMake sure your token has the correct permissions.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    465\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    468\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m response.status_code == \u001b[32m416\u001b[39m:\n\u001b[32m    469\u001b[39m     range_header = response.request.headers.get(\u001b[33m\"\u001b[39m\u001b[33mRange\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: (Request ID: Root=1-6942c6a2-163359762956f17153375d1c;4ede74df-bb54-44f7-809f-0faabb428338)\n\n403 Forbidden: You don't have the rights to create a model under the namespace \"david-barnes\".\nCannot access content at: https://huggingface.co/api/repos/create.\nMake sure your token has the correct permissions."
     ]
    }
   ],
   "source": [
    "# First, reinstall with compatible versions (no downgrade needed)\n",
    "# !uv pip install -q peft transformers huggingface_hub torch\n",
    "\n",
    "# Then run the script\n",
    "%run merge_and_push.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "646b16a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m44 packages\u001b[0m \u001b[2min 148ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m                                                \n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m                                  \u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.12.0\u001b[0m\n",
      "‚úÖ Done! RESTART YOUR KERNEL NOW!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Upgrade transformers (restart kernel after this!)\n",
    "!uv pip install -U transformers peft huggingface_hub\n",
    "print(\"‚úÖ Done! RESTART YOUR KERNEL NOW!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1e0cd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping import of cpp extensions due to incompatible torch version 2.9.1+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Logged in with HF_TOKEN\n",
      "üìÇ Loading adapter config...\n",
      "   Base model: unsloth/Ministral-3-3B-Instruct-2512\n",
      "üîß Loading base model in float16 (this may take ~6GB VRAM)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:05<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "One of the tokens is not a string or an AddedToken",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/beancount_projects/merge_and_push.py:42\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Step 3: Load tokenizer\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müìù Loading tokenizer...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Step 4: Apply the adapter\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müîó Applying LoRA adapter...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/beancount_projects/.venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:1156\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1152\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1153\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1154\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not exist or is not currently imported.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1155\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[32m   1159\u001b[39m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[32m   1160\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/beancount_projects/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2113\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2110\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2111\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2113\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2116\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2117\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2121\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2122\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2124\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2125\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/beancount_projects/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2359\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2357\u001b[39m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[32m   2358\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2359\u001b[39m     tokenizer = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2360\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[32m   2361\u001b[39m     logger.info(\n\u001b[32m   2362\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2363\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2364\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/beancount_projects/.venv/lib/python3.12/site-packages/transformers/models/llama/tokenization_llama_fast.py:154\u001b[39m, in \u001b[36mLlamaTokenizerFast.__init__\u001b[39m\u001b[34m(self, vocab_file, tokenizer_file, clean_up_tokenization_spaces, unk_token, bos_token, eos_token, add_bos_token, add_eos_token, use_default_system_prompt, legacy, add_prefix_space, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m add_prefix_space \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    152\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mfrom_slow\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m    \u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m    \u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_bos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_bos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_eos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_eos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_default_system_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_default_system_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_prefix_space\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_prefix_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlegacy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlegacy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[38;5;28mself\u001b[39m._add_bos_token = add_bos_token\n\u001b[32m    169\u001b[39m \u001b[38;5;28mself\u001b[39m._add_eos_token = add_eos_token\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/beancount_projects/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:178\u001b[39m, in \u001b[36mPreTrainedTokenizerFast.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    175\u001b[39m     kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mpad_to_multiple_of\u001b[39m\u001b[33m\"\u001b[39m, _padding[\u001b[33m\"\u001b[39m\u001b[33mpad_to_multiple_of\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    177\u001b[39m \u001b[38;5;66;03m# We call this after having initialized the backend tokenizer because we update it.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[38;5;28mself\u001b[39m._tokenizer.encode_special_tokens = \u001b[38;5;28mself\u001b[39m.split_special_tokens\n\u001b[32m    181\u001b[39m added_tokens_decoder_hash = {\u001b[38;5;28mhash\u001b[39m(\u001b[38;5;28mrepr\u001b[39m(token)) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.added_tokens_decoder}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/beancount_projects/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1469\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m   1464\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.chat_template, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m   1465\u001b[39m     \u001b[38;5;66;03m# Chat templates are stored as lists of dicts with fixed key names,\u001b[39;00m\n\u001b[32m   1466\u001b[39m     \u001b[38;5;66;03m# we reconstruct that into a single dict while loading them.\u001b[39;00m\n\u001b[32m   1467\u001b[39m     \u001b[38;5;28mself\u001b[39m.chat_template = {template[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]: template[\u001b[33m\"\u001b[39m\u001b[33mtemplate\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m template \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chat_template}\n\u001b[32m-> \u001b[39m\u001b[32m1469\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1471\u001b[39m \u001b[38;5;28mself\u001b[39m.extra_special_tokens = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mextra_special_tokens\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m   1472\u001b[39m \u001b[38;5;28mself\u001b[39m._set_model_specific_special_tokens(special_tokens=\u001b[38;5;28mself\u001b[39m.extra_special_tokens)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/beancount_projects/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:902\u001b[39m, in \u001b[36mSpecialTokensMixin.__init__\u001b[39m\u001b[34m(self, verbose, **kwargs)\u001b[39m\n\u001b[32m    900\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33madditional_special_tokens\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    901\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mValue \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not a list or tuple\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m902\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(t, (\u001b[38;5;28mstr\u001b[39m, AddedToken)) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m value), (\n\u001b[32m    903\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOne of the tokens is not a string or an AddedToken\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    904\u001b[39m     )\n\u001b[32m    905\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value)\n\u001b[32m    906\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, (\u001b[38;5;28mstr\u001b[39m, AddedToken)):\n",
      "\u001b[31mAssertionError\u001b[39m: One of the tokens is not a string or an AddedToken"
     ]
    }
   ],
   "source": [
    "# Cell 2: Run the merge\n",
    "%run merge_and_push.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
