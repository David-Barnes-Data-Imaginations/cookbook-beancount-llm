{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a4a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs Unsloth, Xformers (Flash Attention), and TRL\n",
    "!uv pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!uv pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a4de73",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -q -U bitsandbytes\n",
    "# Use stable releases instead of git main to avoid breaking changes\n",
    "# !uv pip install -q -U \"transformers>=4.36.0,<4.50.0\"  # Stable version that works with bnb, \n",
    "# !uv pip install transformers -U\n",
    "# changed from 'git+https://github.com/huggingface/transformers.git'\n",
    "!uv pip install -q -U \"peft>=0.7.0\"\n",
    "!uv pip install -q -U \"accelerate>=0.25.0\"\n",
    "!uv pip install -q datasets\n",
    "!uv pip install -q pandas\n",
    "!uv pip install -q tensorboard\n",
    "!uv pip install -q -U \"huggingface-hub>=0.34.0,<1.0\"\n",
    "!uv pip install -q trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6daa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install torch torchvision\n",
    "# !uv pip install \"transformers>=5.0.0rc1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86818ea5",
   "metadata": {},
   "source": [
    "### Create Inference Harness\n",
    "\n",
    "The next two cells are just to create a simple inference harness which we will use to do quick evals whilst we review our checkpoints. They are nothing to do with training the model and we‚Äôll revisit later.\n",
    "\n",
    "---\n",
    "\n",
    "The line `os.environ['TOKENIZERS_PARALLELISM'] = 'false'` is just to stop warnings where HF tokens use multiple CPU cores by default\n",
    "\n",
    "When combined with PyTorch's DataLoader (which also uses multiprocessing), you can get conflicts so we set to `False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a08ae03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  pre {\n",
       "      white-space: pre-wrap;\n",
       "  }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Apply CSS to enable text wrapping in code output\n",
    "display(HTML('''\n",
    "<style>\n",
    "  pre {\n",
    "      white-space: pre-wrap;\n",
    "  }\n",
    "</style>\n",
    "'''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab47cb9",
   "metadata": {},
   "source": [
    "Set up the transformers inference API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6c7861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(query: str, model, tokenizer) -> str:\n",
    "  device = \"cuda:0\"\n",
    "\n",
    "  # IMPORTANT: Must match training format exactly!\n",
    "  # Training uses \"### Instruction:\" and \"### Response:\", not \"Question/Answer\"\n",
    "  prompt_template = \"\"\"### Instruction:\n",
    "{query}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "  prompt = prompt_template.format(query=query)\n",
    "\n",
    "  encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "  model_inputs = encodeds.to(device)\n",
    "\n",
    "\n",
    "  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "  decoded = tokenizer.batch_decode(generated_ids)\n",
    "  return (decoded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2f45e9",
   "metadata": {},
   "source": [
    "### 3. Model & Tokenizer loading \n",
    "\n",
    "We'll load the model using **QLoRA** quantization to reduce the usage of memory\n",
    "In full fine-tuning:\n",
    "Our optimizer **AdamW** updates every weight matrix in the neural network.\n",
    "\n",
    "\n",
    "We use FastLanguageModel here.\n",
    "\n",
    "## Important: \n",
    "\n",
    "I've set it to load the BF16 Reasoning model in 4-bit mode, which fits perfectly on consumer GPUs while keeping high accuracy.\n",
    "\n",
    "This should change with the instruct fp 8 model i think?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48f7d50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david-barnes/Programs/beancount_projects/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.12.6: Fast Mistral patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4070 SUPER. Num GPUs = 1. Max memory: 11.594 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.33s/it]\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unsloth: The tokenizer is weirdly not loaded? Please check if there is one.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      7\u001b[39m load_in_4bit = \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;66;03m# This replaces your 'bnb_config'\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 2. Load BOTH Model and Tokenizer\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Use Unsloth's pre-converted version!\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m model, tokenizer = \u001b[43mFastLanguageModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43munsloth/Ministral-3-3B-Instruct-2512\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Match the download name!\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/beancount_projects/.venv/lib/python3.12/site-packages/unsloth/models/loader.py:490\u001b[39m, in \u001b[36mFastLanguageModel.from_pretrained\u001b[39m\u001b[34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, load_in_16bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, offload_embedding, float32_mixed_precision, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, qat_scheme, load_in_fp8, unsloth_tiled_mlp, *args, **kwargs)\u001b[39m\n\u001b[32m    471\u001b[39m     dispatch_model = (\n\u001b[32m    472\u001b[39m         FastQwen3Model \u001b[38;5;28;01mif\u001b[39;00m model_type == \u001b[33m\"\u001b[39m\u001b[33mqwen3\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m FastQwen3MoeModel\n\u001b[32m    473\u001b[39m     )\n\u001b[32m    474\u001b[39m \u001b[38;5;66;03m# elif model_type == \"falcon_h1\":\u001b[39;00m\n\u001b[32m    475\u001b[39m \u001b[38;5;66;03m#     dispatch_model = FastFalconH1Model\u001b[39;00m\n\u001b[32m    476\u001b[39m \u001b[38;5;66;03m#     if not SUPPORTS_FALCON_H1:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[38;5;66;03m#     dispatch_model = FastGraniteModel\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFastModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mold_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mload_in_16bit\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_16bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfull_finetuning\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_finetuning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# [TODO] No effect\u001b[39;49;00m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# [TODO] No effect\u001b[39;49;00m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_gradient_checkpointing\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_gradient_checkpointing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresize_model_vocab\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mresize_model_vocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# [TODO] No effect\u001b[39;49;00m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_logits\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Return logits\u001b[39;49;00m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfullgraph\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# No graph breaks\u001b[39;49;00m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_exact_model_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_exact_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_embedding\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43moffload_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfloat32_mixed_precision\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfloat32_mixed_precision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Pass vLLM/inference parameters\u001b[39;49;00m\n\u001b[32m    512\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfloat8_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfloat8_kv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_lora_rank\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_lora_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43mqat_scheme\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mqat_scheme\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m        \u001b[49m\u001b[43mload_in_fp8\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_fp8\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m        \u001b[49m\u001b[43munsloth_tiled_mlp\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43munsloth_tiled_mlp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_gradient_checkpointing == \u001b[33m\"\u001b[39m\u001b[33munsloth\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    526\u001b[39m     patch_unsloth_smart_gradient_checkpointing(dtype = dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/beancount_projects/.venv/lib/python3.12/site-packages/unsloth/models/loader.py:1164\u001b[39m, in \u001b[36mFastModel.from_pretrained\u001b[39m\u001b[34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, load_in_16bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, return_logits, fullgraph, use_exact_model_name, auto_model, whisper_language, whisper_task, unsloth_force_compile, offload_embedding, float32_mixed_precision, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, qat_scheme, load_in_fp8, unsloth_tiled_mlp, *args, **kwargs)\u001b[39m\n\u001b[32m   1161\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m auto_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1162\u001b[39m     auto_model = AutoModelForVision2Seq \u001b[38;5;28;01mif\u001b[39;00m is_vlm \u001b[38;5;28;01melse\u001b[39;00m AutoModelForCausalLM\n\u001b[32m-> \u001b[39m\u001b[32m1164\u001b[39m model, tokenizer = \u001b[43mFastBaseModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m_get_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_in_16bit\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_16bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfull_finetuning\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_finetuning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_peft\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_types\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauto_model\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_gradient_checkpointing\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_gradient_checkpointing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m    \u001b[49m\u001b[43msupports_sdpa\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43msupports_sdpa\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhisper_language\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhisper_language\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhisper_task\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhisper_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauto_config\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m    \u001b[49m\u001b[43moffload_embedding\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43moffload_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfloat32_mixed_precision\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfloat32_mixed_precision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Pass vLLM/inference parameters\u001b[39;49;00m\n\u001b[32m   1187\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfloat8_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfloat8_kv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_lora_rank\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_lora_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resize_model_vocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1198\u001b[39m     model.resize_token_embeddings(resize_model_vocab)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/beancount_projects/.venv/lib/python3.12/site-packages/unsloth/models/vision.py:853\u001b[39m, in \u001b[36mFastBaseModel.from_pretrained\u001b[39m\u001b[34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, load_in_16bit, full_finetuning, token, device_map, trust_remote_code, model_types, tokenizer_name, auto_model, use_gradient_checkpointing, supports_sdpa, whisper_language, whisper_task, auto_config, offload_embedding, float32_mixed_precision, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, unsloth_vllm_standby, **kwargs)\u001b[39m\n\u001b[32m    851\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    852\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m model\n\u001b[32m--> \u001b[39m\u001b[32m853\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    854\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnsloth: The tokenizer is weirdly not loaded? Please check if there is one.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    855\u001b[39m     )\n\u001b[32m    856\u001b[39m patch_saving_functions(tokenizer, vision = \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    858\u001b[39m \u001b[38;5;66;03m# Fix gradient accumulation\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Unsloth: The tokenizer is weirdly not loaded? Please check if there is one."
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# 1. Configuration\n",
    "max_seq_length = 2048\n",
    "dtype = None # Auto-detects your GPU capabilities\n",
    "load_in_4bit = True # This replaces your 'bnb_config'\n",
    "\n",
    "# 2. Load BOTH Model and Tokenizer\n",
    "# Use Unsloth's pre-converted version!\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Ministral-3-3B-Instruct-2512\",  # Match the download name!\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed046041",
   "metadata": {},
   "source": [
    "### 3. Configure LoRA:\n",
    "\n",
    "Unsloth handles the target modules automatically (including the tricky gate_proj, up_proj, etc. that vanilla Peft requires you to list manually)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176ad2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e156dbf",
   "metadata": {},
   "source": [
    "4. Data Loading & Formatting (The \"Junior Accountant\" Logic):\n",
    "\n",
    "This is where we inject your specific \"Junior Accountant\" System Prompt.\n",
    "\n",
    "It maps your refined_data.json to the Mistral chat format automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce9ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Define your custom System Prompt\n",
    "system_prompt = \"\"\"You are an expert accountant using Beancount syntax.\n",
    "Instructions:\n",
    "1. Analyze the transaction and the historical <context>.\n",
    "2. FORMULATE A PLAN inside <plan> tags. Decide the high-level category (Asset, Liability, Income, Expense) and the double-entry logic.\n",
    "3. EXECUTE THE PLAN inside <reasoning> tags. Verify the account name against history and confirm the math balances to zero.\n",
    "4. WRITE THE CODE inside <entry> tags. Use strict Beancount syntax.\n",
    "IMPORTANT: Output ONLY the raw XML.\"\"\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    conversations = []\n",
    "    \n",
    "    # We assume your JSON has 'data.prompt' (input) and 'predictions...text' (output)\n",
    "    # You might need to adjust these keys based on exactly how Label Studio exported the JSON\n",
    "    # This example assumes a flat format: {\"prompt\": \"...\", \"response\": \"...\"}\n",
    "    # If using raw Label Studio export, let me know and I can tweak this extraction!\n",
    "    \n",
    "    for prompt, response in zip(examples['prompt'], examples['response']):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "            {\"role\": \"assistant\", \"content\": response}\n",
    "        ]\n",
    "        \n",
    "        # Apply the chat template (Correctly handles [INST] tags)\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "        conversations.append(text)\n",
    "        \n",
    "    return { \"text\" : conversations }\n",
    "\n",
    "# Load your dataset\n",
    "# Make sure 'refined_data.json' is formatted with 'prompt' and 'response' fields!\n",
    "dataset = load_dataset(\"json\", data_files=\"final_train.json\", split=\"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dcb85c",
   "metadata": {},
   "source": [
    "## Critical Check: JSON Format\n",
    "Label Studio exports JSON in a nested format (inside predictions, result, etc.), but load_dataset usually expects a flat list of {\"prompt\": \"...\", \"response\": \"...\"}.\n",
    "\n",
    "Before running this, run a quick Python script to flatten your refined_data.json into a train.json for Unsloth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6067a5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "input_file = \"final_train.json\"\n",
    "output_file = \"ready_to_train.json\"\n",
    "\n",
    "print(f\"üìñ Reading {input_file}...\")\n",
    "with open(input_file, \"r\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "flat_data = []\n",
    "skipped_count = 0\n",
    "\n",
    "for item in raw_data:\n",
    "    try:\n",
    "        # 1. Extract Prompt\n",
    "        prompt = item['data']['prompt']\n",
    "        \n",
    "        # 2. Extract Response (CRITICAL CHANGE: Look in 'annotations', not 'predictions')\n",
    "        # The Senior Accountant saves the final version in 'annotations'\n",
    "        response_text = item['annotations'][0]['result'][0]['value']['text'][0]\n",
    "        \n",
    "        # 3. AUTO-CLEANUP: Fix the \"Space after Colon\" bug\n",
    "        # Claude wrote \"Assets: Lloyds:Checking\", but Beancount prefers \"Assets:Lloyds:Checking\"\n",
    "        # This regex removes the space after the colon for the 5 root account types\n",
    "        response_text = re.sub(r'(Assets|Liabilities|Expenses|Income|Equity):\\s+', r'\\1:', response_text)\n",
    "        \n",
    "        flat_data.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"response\": response_text\n",
    "        })\n",
    "        \n",
    "    except (KeyError, IndexError) as e:\n",
    "        # This catches any malformed records\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "\n",
    "# 4. Save flattened file\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(flat_data, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Success! Processed {len(flat_data)} records.\")\n",
    "if skipped_count > 0:\n",
    "    print(f\"‚ö†Ô∏è Skipped {skipped_count} malformed records.\")\n",
    "print(f\"üíæ Saved to {output_file} - You are ready to train!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cde9498",
   "metadata": {},
   "source": [
    "### Check where the model is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098082b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check where the model is cached\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "\n",
    "cache_dir = os.path.expanduser(\"~/.cache/huggingface/hub/\")\n",
    "print(f\"Model cache location: {cache_dir}\")\n",
    "print(\"\\nContents:\")\n",
    "if os.path.exists(cache_dir):\n",
    "    for item in os.listdir(cache_dir)[:10]:  # Show first 10 items\n",
    "        print(f\"  - {item}\")\n",
    "else:\n",
    "    print(\"Cache directory not found yet\")\n",
    "\n",
    "# You can also set a custom cache location if you prefer:\n",
    "# os.environ['HF_HOME'] = '/path/to/custom/cache'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc77db5",
   "metadata": {},
   "source": [
    "## Apply QLora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732dd1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from trl import SFTTrainer, SFTConfig  # Changed import\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,\n",
    "    args = SFTConfig(  # Changed from TrainingArguments to SFTConfig\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        # These are now part of SFTConfig\n",
    "        max_seq_length = max_seq_length,\n",
    "        dataset_text_field = \"text\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Start Training\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4381ced9",
   "metadata": {},
   "source": [
    "Quick calculation:\n",
    "\n",
    "700 records\n",
    "Effective batch size = per_device_batch_size (2) √ó gradient_accumulation_steps (4) = 8\n",
    "Steps per epoch = 700 / 8 = ~88 steps\n",
    "So 60 steps = ~0.7 epochs - you haven't even completed one full pass through your data yet!\n",
    "\n",
    "Recommendations:\n",
    "\n",
    "Epochs |\tSteps |\tUse Case |\n",
    "1 |\t~90 |\tMinimum - sees all data once |\n",
    "2-3 |\t~180-270|\tSweet spot for fine-tuning |\n",
    "5+ |\t440+ |\tRisk of overfitting |\n",
    "\n",
    "Since your loss was still decreasing at step 60, you probably have room to train more. I'd suggest trying max_steps = 180 (about 2 epochs) for a good balance.\n",
    "\n",
    "Watch for:\n",
    "\n",
    "‚úÖ Good sign: Loss continues decreasing smoothly\n",
    "‚ö†Ô∏è Overfitting warning: Loss drops very low (<0.1) or starts fluctuating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402268d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training from step 60 to step 180\n",
    "trainer.args.max_steps = 180  # New target\n",
    "\n",
    "# Resume from the last checkpoint\n",
    "trainer_stats = trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5d57da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training from step 60 to step 180\n",
    "trainer.args.max_steps = 270  # New target\n",
    "\n",
    "# Resume from the last checkpoint\n",
    "trainer_stats = trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00db7b43",
   "metadata": {},
   "source": [
    "### LOGIN TO HUB\n",
    "\n",
    "When we push to HuggingFace Hub, it will merge our local QLoRa adaptor with the base model we used to train, on the hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11efb6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Try to login with token from environment variable\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "if hf_token:\n",
    "\tlogin(token=hf_token)\n",
    "\tprint(\"Logged in with HF_TOKEN environment variable\") \n",
    "else:\n",
    "\t# Skip login for local training - you can still train without pushing to hub\n",
    "\tprint(\"No HF_TOKEN found. Proceeding with local training on local GPU...\")\n",
    "\tprint(\"Note: You won't be able to push models to HuggingFace Hub without authentication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52519998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import os\n",
    "device = \"cuda:0\"\n",
    "\n",
    "# 1. CONFIGURATION\n",
    "# Point this to the exact folder on your disk\n",
    "checkpoint_path = \"outputs/checkpoint-180\" \n",
    "repo_name = \"DataImaginations/ministral-3B-Beancount-v1\" # Your Hugging Face repo\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "\n",
    "# 2. LOAD SPECIFIC CHECKPOINT\n",
    "# Unsloth is smart: if you point it to a folder, it loads the base model \n",
    "# AND applies the adapters from that folder automatically.\n",
    "print(f\"üìÇ Loading checkpoint from {checkpoint_path}...\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = checkpoint_path, \n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True, # Keep True for fast loading (Unsloth handles the merge magic)\n",
    ")\n",
    "\n",
    "# 3. MERGE & PUSH\n",
    "# This will de-quantize the base model, merge your checkpoint-180 adapters, \n",
    "# and upload a clean 16-bit model to the Hub.\n",
    "print(f\"üöÄ Merging and pushing to {repo_name}...\")\n",
    "\n",
    "model.push_to_hub_merged(\n",
    "    repo_name,\n",
    "    tokenizer,\n",
    "    save_method = \"merged_16bit\", # options: \"merged_4bit\", \"merged_16bit\"\n",
    "    token = hf_token\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Done! Your Junior Accountant (Checkpoint 180) is live!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b4fe6c",
   "metadata": {},
   "source": [
    "## ALL EDITS BELOW ARE PURELY ME IN PACKAGE HELL AFTER USING A MODEL SO NEW THAT TRANSFORMERS AND UNSLOTH DON'T MATCH\n",
    "\n",
    "### IF YOU'RE USING THIS 1 WEEK+ AFTER 18/12/2025 YOU WON'T NEED THE BELOW (WHICH DIDN'T WORK ANYWAY)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a54c04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip uninstall unsloth unsloth_zoo\n",
    "!uv pip uninstall unsloth unsloth_zoo  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f309ca6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unsloth\n",
    "# This SHOULD fail with ModuleNotFoundError. If it doesn't, manual deletion is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57704ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies FIRST\n",
    "!uv pip install \"transformers==4.46.3\" \"peft==0.13.2\" \"trl==0.8.6\" \"accelerate==0.34.2\" \"huggingface_hub>=0.26.0\" \"bitsandbytes==0.44.1\"\n",
    "# Install Unsloth Stable (PyPI version, NOT git)\n",
    "!uv pip install \"unsloth==2024.11.7\"  # November stable release known to work with TR 4.46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec81e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Uninstall existing packages to prevent conflicts\n",
    "!uv pip uninstall transformers peft trl unsloth accelerate\n",
    "# 2. Install \"Known Good\" compatible versions (Late 2024 Stable Stack)\n",
    "# - Transformers 4.46.3: Stable, works with Unsloth and HF Hub\n",
    "# - PEFT 0.13.2: Compatible with Tr 4.46\n",
    "# - TRL 0.8.6: Compatible with Tr 4.46\n",
    "# - Accelerate 0.34.2: Stable backend\n",
    "!uv pip install \"transformers==4.46.3\" \"peft==0.13.2\" \"trl==0.8.6\" \"accelerate==0.34.2\" \"huggingface_hub>=0.26.0\" \"bitsandbytes==0.44.1\"\n",
    "# 3. Install Unsloth WITHOUT upgrading dependencies\n",
    "# This forces it to use our pinned versions\n",
    "!uv pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" --no-deps\n",
    "# 2. Restart your kernel is REQUIRED after this!\n",
    "print(\"Please restart your kernel now!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d1a3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install \"bitsandbytes==0.44.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcbb74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -q -U bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435fb190",
   "metadata": {},
   "source": [
    "# Push Model to hub!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ed54f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Try to login with token from environment variable\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "if hf_token:\n",
    "\tlogin(token=hf_token)\n",
    "\tprint(\"Logged in with HF_TOKEN environment variable\") \n",
    "else:\n",
    "\t# Skip login for local training - you can still train without pushing to hub\n",
    "\tprint(\"No HF_TOKEN found. Proceeding with local training on local GPU...\")\n",
    "\tprint(\"Note: You won't be able to push models to HuggingFace Hub without authentication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295378f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, MistralConfig\n",
    "try:\n",
    "    AutoConfig.register(\"mistral3\", MistralConfig)\n",
    "    AutoConfig.register(\"ministral3\", MistralConfig)\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5427de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import os\n",
    "device = \"cuda:0\"\n",
    "\n",
    "# 1. CONFIGURATION\n",
    "# Point this to the exact folder on your disk\n",
    "checkpoint_path = \"outputs/checkpoint-180\" \n",
    "repo_name = \"DataImaginations/ministral-3B-Beancount-v1\" # Your Hugging Face repo\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "\n",
    "# 2. LOAD SPECIFIC CHECKPOINT\n",
    "# Unsloth is smart: if you point it to a folder, it loads the base model \n",
    "# AND applies the adapters from that folder automatically.\n",
    "print(f\"üìÇ Loading checkpoint from {checkpoint_path}...\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = checkpoint_path, \n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True, # Keep True for fast loading (Unsloth handles the merge magic)\n",
    ")\n",
    "\n",
    "# 3. MERGE & PUSH\n",
    "# This will de-quantize the base model, merge your checkpoint-180 adapters, \n",
    "# and upload a clean 16-bit model to the Hub.\n",
    "print(f\"üöÄ Merging and pushing to {repo_name}...\")\n",
    "\n",
    "model.push_to_hub_merged(\n",
    "    repo_name,\n",
    "    tokenizer,\n",
    "    save_method = \"merged_16bit\", # options: \"merged_4bit\", \"merged_16bit\"\n",
    "    token = hf_token\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Done! Your Junior Accountant (Checkpoint 180) is live!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4bf6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Completely clean slate\n",
    "!uv pip uninstall transformers unsloth peft trl accelerate bitsandbytes\n",
    "\n",
    "# 2. Install the KNOWN WORKING stack from late 2024\n",
    "!uv pip install \"transformers==4.46.3\" \"peft==0.13.2\" \"trl==0.8.6\" \"accelerate==0.34.2\" \"bitsandbytes==0.44.1\" \"huggingface_hub>=0.26.0\"\n",
    " \n",
    "# 3. Install Unsloth WITHOUT letting it upgrade dependencies\n",
    "!uv pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" --no-deps\n",
    "\n",
    "print(\"‚úÖ Done! RESTART YOUR KERNEL NOW before running any other cells!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88417a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import os\n",
    "\n",
    "checkpoint_path = \"outputs/checkpoint-180\"\n",
    "repo_name = \"DataImaginations/ministral-3B-Beancount-v1\"\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=checkpoint_path,\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "model.push_to_hub_merged(\n",
    "    repo_name,\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\",\n",
    "    token=hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a38324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load base model\n",
    "base_model_name = \"unsloth/Ministral-3-3B-Instruct-2512\"  # Or the Unsloth version\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "# Load and merge adapter\n",
    "model = PeftModel.from_pretrained(model, \"outputs/checkpoint-180\")\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Push to hub\n",
    "model.push_to_hub(\"DataImaginations/ministral-3B-Beancount-v1\")\n",
    "tokenizer.push_to_hub(\"DataImaginations/ministral-3B-Beancount-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49df4d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, reinstall with compatible versions (no downgrade needed)\n",
    "# !uv pip install -q peft transformers huggingface_hub torch\n",
    "\n",
    "# Then run the script\n",
    "%run merge_and_push.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646b16a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Upgrade transformers (restart kernel after this!)\n",
    "!uv pip install -U transformers peft huggingface_hub\n",
    "print(\"‚úÖ Done! RESTART YOUR KERNEL NOW!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e0cd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Run the merge\n",
    "%run merge_and_push.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
